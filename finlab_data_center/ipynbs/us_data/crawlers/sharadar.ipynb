{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pycharm 測試\n",
    "import sys,os\n",
    "sys.path.append(\"..\")\n",
    "import django\n",
    "django.setup()\n",
    "import quandl\n",
    "from django.conf import settings\n",
    "from us_data.models import *\n",
    "from etl.import_sql import *\n",
    "import logging\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharadarCrawler:\n",
    "    def __init__(self, date=None):\n",
    "        self.date = date\n",
    "        quandl.ApiConfig.api_key = settings.CONFIG_DATA.get(\"QUANDL_API_KEY\")\n",
    "\n",
    "    def download_sep(self):\n",
    "        df = quandl.get_table('SHARADAR/SEP', date=self.date, paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        df = df.drop(columns='lastupdated')\n",
    "        df = df.rename(columns={'ticker': 'stock_id'})\n",
    "        return df\n",
    "\n",
    "    def download_sp500(self):\n",
    "        engine = create_connect_engine(SharaderSp500)\n",
    "        conn = engine.connect()\n",
    "        db_name=get_connect_db_name(SharaderSp500)\n",
    "        sql = f\"DELETE FROM {db_name}.sharader_sp500\"\n",
    "        conn.execute(sql)\n",
    "        df = quandl.get_table('SHARADAR/SP500', paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        df = df.drop(columns='note')\n",
    "        df = df.rename(columns={'ticker': 'stock_id'})\n",
    "        df = df.replace({'N/A': None})\n",
    "        df = df[df['action'] == 'current']\n",
    "        df['id'] = [i for i in range(1, len(df) + 1)]\n",
    "        return df\n",
    "\n",
    "    def download_sf1(self):\n",
    "        df = quandl.get_table('SHARADAR/SF1', calendardate=self.date, paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        df = df.rename(columns={'ticker': 'stock_id', \"calendardate\": 'date'})\n",
    "        return df\n",
    "\n",
    "    def download_sf3a(self):\n",
    "        df = quandl.get_table('SHARADAR/SF3A', calendardate=self.date, paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        df = df.rename(columns={'ticker': 'stock_id', \"calendardate\": 'date'})\n",
    "        return df\n",
    "\n",
    "    def download_sf3b(self):\n",
    "        df = quandl.get_table('SHARADAR/SF3B', calendardate=self.date, paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        df = df.rename(columns={'calendardate': 'date'})\n",
    "        return df\n",
    "\n",
    "    def download_actions(self):\n",
    "        df = quandl.get_table('SHARADAR/actions', date=self.date, paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        df = df.rename(columns={'ticker': 'stock_id', 'calendardate': 'date'})\n",
    "        df = df.replace({'N/A': None})\n",
    "        return df\n",
    "\n",
    "    def download_daily(self):\n",
    "        df = quandl.get_table('SHARADAR/DAILY', date=self.date, paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        df = df.drop(columns='lastupdated')\n",
    "        df = df.rename(columns={'ticker': 'stock_id'})\n",
    "        return df\n",
    "\n",
    "    def download_eventcodes(self):\n",
    "        df = quandl.get_table('SHARADAR/EVENTCODES', paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        #         df=df.rename(columns={'ticker': 'stock_id','calendardate':'date'})\n",
    "        #         df=df.replace({'N/A': None})\n",
    "        return df\n",
    "\n",
    "    def download_tickers(self):\n",
    "        df = quandl.get_table('SHARADAR/TICKERS', paginate=True)\n",
    "        if len(df) < 1:\n",
    "            return None\n",
    "        return df\n",
    "\n",
    "\n",
    "    \n",
    "# df=SharadarCrawler(['2020-6-16','2020-10-12']).download_sep() \n",
    "# df=SharadarCrawler().download_sp500()\n",
    "# df=SharadarCrawler(['2020-06-30']).download_sf1()\n",
    "# df=SharadarCrawler(['2018-03-31']).download_sf3b()\n",
    "# df=SharadarCrawler(['2020-08-31']).download_actions()\n",
    "# df=SharadarCrawler(['2020-08-31']).download_daily()\n",
    "# df=SharadarCrawler(['2020-08-31']).download_tickers()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日財報爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取2020-09-01到2020-09-10\n",
    "crawler=CrawlerProcess(SharadarCrawler,'download_sep',SharadarSep,'date_range',use_date_list=True)\n",
    "# print(crawler)\n",
    "\n",
    "# # 指定日期區間爬蟲\n",
    "# crawler.specified_date_crawl('2020-6-16','2020-10-12')\n",
    "\n",
    "# 自動檢查最後日期爬取\n",
    "crawler.auto_update_crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  季財報爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# season generator\n",
    "# def us_seasonly( start_date, end_date):\n",
    "#     start_date=datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "#     end_date=datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "#     present_year = datetime.datetime.now()\n",
    "#     calendardates = pd.date_range(str(start_date.year-1), str(end_date.year+1), freq='Q').astype(str).to_list()\n",
    "#     return calendardates\n",
    "\n",
    "# us_seasonly( '2018-09-01','2020-09-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharadar_sf1 \n",
      "table_earliest_date:1990-06-30 00:00:00\n",
      "table_latest_date:2020-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:etl.import_sql:Finish!download data from 2019-09-30 to 2019-09-30\n",
      "INFO:etl.import_sql:Finish!download data from 2019-12-31 to 2019-12-31\n",
      "INFO:etl.import_sql:Finish!download data from 2020-03-31 to 2020-03-31\n",
      "INFO:etl.import_sql:Finish!download data from 2020-06-30 to 2020-06-30\n",
      "INFO:etl.import_sql:Finish!<class 'us_data.models.SharadarSf1'> date:from 2020-09-30 to 2020-09-30 bulk_create:14416\n",
      "INFO:etl.import_sql:Finish!<class 'us_data.models.SharadarSf1'> date:from 2020-09-30 to 2020-09-30 bulk_update:0\n",
      "INFO:etl.import_sql:Finish!download data from 2020-09-30 to 2020-09-30\n",
      "INFO:etl.import_sql:Finish!<class 'us_data.models.SharadarSf1'> date:from 2020-12-31 to 2020-12-31 bulk_create:306\n",
      "INFO:etl.import_sql:Finish!<class 'us_data.models.SharadarSf1'> date:from 2020-12-31 to 2020-12-31 bulk_update:0\n",
      "INFO:etl.import_sql:Finish!download data from 2020-12-31 to 2020-12-31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finish!download data from 2019-09-30 to 2020-12-31'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler=CrawlerProcess(SharadarCrawler,'download_sf1',SharadarSf1,'us_season_range',time_sleep=0,use_date_list=False, jump_update=True)\n",
    "print(crawler)\n",
    "# # 指定日期區間爬蟲\n",
    "# crawler.specified_date_crawl('2020-09-01','2020-09-06')\n",
    "\n",
    "# 自動採當今日期前後爬取\n",
    "crawler.auto_update_crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_task_once(crawler, func, model):\n",
    "    df = getattr(eval(crawler)(), func)()\n",
    "    importer = SqlImporter.add_to_sql(eval(model), df)\n",
    "    return importer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_task_once('SharadarCrawler','download_sp500','SharaderSp500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nasdaq 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlickchartsCrawler:\n",
    "    def __init__(self, date=None):\n",
    "        self.market_range=['dowjones','nasdaq100','sp500']\n",
    "    def index_components(self,index_name:str):\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                          'Chrome/86.0.4240.75 Safari/537.36'}\n",
    "        # dowjones,nasdaq100,sp500\n",
    "        try:\n",
    "            url=f\"https://www.slickcharts.com/{index_name}\"\n",
    "        except Exception as error:\n",
    "            logger.error(error)\n",
    "            df = pd.DataFrame()\n",
    "            return df\n",
    "        r = requests.get(url, headers=headers)\n",
    "        df = pd.read_html(StringIO(r.text))[0]\n",
    "        df=df.rename(columns={'Company':'name','Symbol': 'stock_id','Weight':'weight'})\n",
    "        df=df[['stock_id','name','weight']]\n",
    "        df['index_name']=index_name\n",
    "        return df\n",
    "    def main(self):\n",
    "        try:\n",
    "            df=pd.concat([self.index_components(i) for i in self.market_range])\n",
    "            df['id']=[i for i in range(1,len(df)+1)]\n",
    "        except Exception as error:\n",
    "            logger.error(error)\n",
    "            return None\n",
    "        engine = create_connect_engine(IndexComponents)\n",
    "        conn = engine.connect()\n",
    "        sql=f\"DELETE FROM dev_us_data.index_components\"\n",
    "        conn.execute(sql)        \n",
    "        return df\n",
    "            \n",
    "\n",
    "# SlickchartsCrawler().main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_task_once('SlickchartsCrawler','main','IndexComponents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## once import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from etl.import_sql import SqlImporter, CrawlerProcess\n",
    "# from us_data.crawlers import *\n",
    "# from us_data.models import *\n",
    "\n",
    "# def crawler_task_once(crawler, func, model):\n",
    "#     df = getattr(eval(crawler)(), func)()\n",
    "#     importer = SqlImporter.add_to_sql(eval(model), df)\n",
    "#     return importer\n",
    "\n",
    "# crawler_task_once('SharadarCrawler','download_sp500','SharaderSp500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,i in enumerate(df):\n",
    "    if len(i)>20:\n",
    "        print(a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_connect_engine(SharaderSp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'a':[1,2,3,4],'b':[4,2,3,4]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(pd.to_numeric)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'=\"0050\"'.replace('=', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
